{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Naive Bayes classifier</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import lib's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1737,
     "status": "ok",
     "timestamp": 1631342018251,
     "user": {
      "displayName": "201951163 TUSHAR",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00030064940507621496"
     },
     "user_tz": -330
    },
    "id": "l3NB8ilK0iYK"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import gc\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3958,
     "status": "ok",
     "timestamp": 1631342023892,
     "user": {
      "displayName": "201951163 TUSHAR",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00030064940507621496"
     },
     "user_tz": -330
    },
    "id": "y3wSAnvF3C44"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_count_mapper = np.zeros((20, 20)) # since i know total number of docs I'm making this array directly\n",
    "class_docs = {} # how much docs each class has\n",
    "train_y = [] # training labels (1-20)\n",
    "conf_matrix = np.zeros((20, 20))\n",
    "\n",
    "with open('train.label', 'r') as fp:\n",
    "  for line in fp:\n",
    "    label = int(line.strip())\n",
    "    # its good to point out that doc_id 'n' will have label at 'n-1'th place\n",
    "    # in simple words doc0 will have label at train_y[0] which will be between 1-20\n",
    "    train_y.append(label)\n",
    "    class_docs[label] = class_docs.get(label, 0) + 1\n",
    "    \n",
    "previous_doc = 1\n",
    "freq = {} # (word, class): count\n",
    "with open('train.data', 'r') as fp:\n",
    "  word_ids = [] # this will basically contain the words and their counts\n",
    "  for line in fp:\n",
    "    doc_id, word_id, count = map(int, line.strip().split(' '))\n",
    "    # this means we have got all the words of our doc now we can take care of frequencies\n",
    "    # for doc1\n",
    "    # (1,5), (2,10), ...\n",
    "    # doc1 => class1\n",
    "    if previous_doc != doc_id:\n",
    "      # doc N will have label at (N-1)th place\n",
    "      class_of_doc = train_y[previous_doc-1] # getting class of doc\n",
    "      for word, word_count in word_ids:\n",
    "        # we are gonna have count of each word in each class in this dictionary\n",
    "        # example. hello, class1 => 20 times\n",
    "        freq[(word, class_of_doc)] = freq.get((word, class_of_doc), 0) + word_count\n",
    "      previous_doc = doc_id\n",
    "      word_ids = [(word_id, count)]\n",
    "    else:\n",
    "      word_ids.append((word_id, count)) # appending word_id\n",
    "# for the last doc\n",
    "class_of_doc = train_y[previous_doc-1] # getting class of doc\n",
    "for word, count in word_ids:\n",
    "  freq[(word, class_of_doc)] = freq.get((word, class_of_doc), 0) + word_count\n",
    "del word_ids\n",
    "\n",
    "vocab = set([pair[0] for pair in freq.keys()])\n",
    "v_len = len(vocab)\n",
    "\n",
    "# finding how much words each class has\n",
    "class_words = {}\n",
    "for pair, word_count in freq.items():\n",
    "  class_ = pair[1]\n",
    "  class_words[class_] = class_words.get(class_, 0) + word_count\n",
    "\n",
    "total_docs = len(train_y) # this should be 11269 if using their indexing (see train.data)\n",
    "# sum(class_docs.values()) # again, this should be same as total_docs\n",
    "\n",
    "prob_class = {}\n",
    "prob_word_class = {}\n",
    "\n",
    "# finding probability of each class\n",
    "for i in class_docs:\n",
    "  prob_class[i] = class_docs[i]/total_docs\n",
    "\n",
    "# findinf probability of each word in each class\n",
    "# we are doing this with smoothing too, for better results\n",
    "for word in vocab:\n",
    "  for class_ in class_words:\n",
    "    freq_class = freq.get((word, class_), 0)\n",
    "    # word/class\n",
    "    prob_word_class[(word, class_)] = (freq_class + 1)/(class_words[class_] + v_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41483,
     "status": "ok",
     "timestamp": 1631342065372,
     "user": {
      "displayName": "201951163 TUSHAR",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00030064940507621496"
     },
     "user_tz": -330
    },
    "id": "uJbPa51ZOZBU",
    "outputId": "0e6e3009-1347-4ffe-a46f-6c37223bed81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7846768820786143\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "conf_matrix = np.zeros((20,20))\n",
    "previous_doc = 1\n",
    "y_expected = []\n",
    "y_actual = []\n",
    "with open('test.label', 'r') as fp:\n",
    "  for line in fp:\n",
    "    y_expected.append(int(line.strip()))\n",
    "total_test_docs = len(y_expected)\n",
    "correct_classified = 0\n",
    "with open('test.data', 'r') as fp:\n",
    "  word_ids = [] # this will basically contain the words and their counts\n",
    "  j = 0\n",
    "  for line in fp:\n",
    "    doc_id, word_id, count = map(int, line.strip().split(' '))\n",
    "    if previous_doc != doc_id:\n",
    "      probs = copy.deepcopy(prob_class)\n",
    "      for i in probs:\n",
    "        probs[i] = np.log(probs[i])\n",
    "      for word, word_count in word_ids:\n",
    "        for class_ in range(1,21):\n",
    "          # print(prob_word_class.get((word, class_), 1e-5), end=' ')\n",
    "          probs[class_] = probs[class_]  +  word_count * np.log(prob_word_class.get((word, class_), 1e-5))\n",
    "          # probs[i] = probs[i] + word_count * p_word_class.get((word, i), 1e-4)\n",
    "      _max_class = 1\n",
    "      _max_val = - np.inf\n",
    "      for i in probs:\n",
    "        if probs[i] > _max_val:\n",
    "          _max_val = probs[i]\n",
    "          _max_class = i\n",
    "      y_actual.append(_max_class)\n",
    "      # print(_max_class, end = ' ')\n",
    "      if y_expected[j] == _max_class:\n",
    "        correct_classified+=1\n",
    "      conf_matrix[_max_class-1][y_expected[j]-1] = conf_matrix[_max_class-1][y_expected[j]-1] + 1\n",
    "      j += 1\n",
    "      previous_doc = doc_id\n",
    "      word_ids = [(word_id, count)]\n",
    "    else:\n",
    "      word_ids.append((word_id, count)) # appending word_id\n",
    "\n",
    "# this is for the last word_ids (code can be taken into function to remove redunduncy)\n",
    "for word, word_count in word_ids:\n",
    "  for class_ in range(1,21):\n",
    "    # print(prob_word_class.get((word, class_), 1e-5), end=' ')\n",
    "    probs[class_] = probs[class_]  +  word_count * np.log(prob_word_class.get((word, class_), 1e-5))\n",
    "    # probs[i] = probs[i] + word_count * p_word_class.get((word, i), 1e-4)\n",
    "_max_class = 1\n",
    "_max_val = - np.inf\n",
    "for i in probs:\n",
    "  if probs[i] > _max_val:\n",
    "    _max_val = probs[i]\n",
    "    _max_class = i\n",
    "# print(_max_class, end = ' ')\n",
    "y_actual.append(_max_class)\n",
    "if y_expected[j] == _max_class:\n",
    "  correct_classified+=1\n",
    "conf_matrix[_max_class-1][y_expected[j]-1] = conf_matrix[_max_class-1][y_expected[j]-1] + 1\n",
    "\n",
    "incorrect_classified = total_test_docs - correct_classified\n",
    "print(correct_classified/total_test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1631342065373,
     "user": {
      "displayName": "201951163 TUSHAR",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00030064940507621496"
     },
     "user_tz": -330
    },
    "id": "oC4w3oTLyWjd",
    "outputId": "9c12f7d4-18d9-4388-cdd6-512ea3b8b659"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7737449470140917"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "arr=f1_score(y_expected, y_actual, average=None);\n",
    "sum(arr)/len(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1631342065374,
     "user": {
      "displayName": "201951163 TUSHAR",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00030064940507621496"
     },
     "user_tz": -330
    },
    "id": "mZ1GTLKXB1Mh",
    "outputId": "b2dfb194-9cc6-4098-e10f-eb697c716ce2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[240.,   3.,   3.,   0.,   1.,   0.,   0.,   0.,   0.,   4.,   2.,\n",
       "          0.,   2.,  10.,   4.,   7.,   1.,  12.,   7.,  47.],\n",
       "       [  0., 296.,  33.,   8.,   8.,  42.,   9.,   1.,   1.,   0.,   0.,\n",
       "          4.,  17.,   7.,   8.,   2.,   0.,   1.,   1.,   2.],\n",
       "       [  0.,   6., 209.,  15.,   9.,   8.,   4.,   0.,   0.,   0.,   0.,\n",
       "          1.,   0.,   1.,   0.,   1.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,  12.,  60., 303.,  36.,  11.,  46.,   2.,   0.,   1.,   0.,\n",
       "          1.,  28.,   3.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   8.,  10.,  22., 277.,   2.,  21.,   0.,   0.,   1.,   0.,\n",
       "          2.,   7.,   0.,   0.,   1.,   1.,   0.,   0.,   0.],\n",
       "       [  1.,  21.,  30.,   2.,   2., 305.,   0.,   1.,   0.,   3.,   0.,\n",
       "          1.,   3.,   0.,   1.,   2.,   0.,   0.,   1.,   0.],\n",
       "       [  0.,   1.,   0.,   5.,   5.,   1., 235.,   5.,   1.,   2.,   0.,\n",
       "          1.,   1.,   0.,   0.,   0.,   1.,   0.,   0.,   0.],\n",
       "       [  0.,   3.,   1.,   6.,   4.,   0.,  31., 356.,  25.,   3.,   1.,\n",
       "          0.,   9.,   4.,   0.,   0.,   3.,   2.,   1.,   0.],\n",
       "       [  0.,   2.,   2.,   0.,   1.,   2.,   5.,   5., 353.,   1.,   0.,\n",
       "          0.,   2.,   0.,   1.,   0.,   1.,   1.,   0.,   1.],\n",
       "       [  0.,   0.,   2.,   0.,   1.,   1.,   0.,   2.,   2., 348.,   4.,\n",
       "          0.,   0.,   1.,   0.,   0.,   1.,   1.,   0.,   0.],\n",
       "       [  1.,   0.,   1.,   1.,   0.,   0.,   1.,   0.,   0.,  16., 383.,\n",
       "          0.,   1.,   0.,   1.,   0.,   1.,   1.,   0.,   0.],\n",
       "       [  1.,  17.,  16.,   5.,   4.,  10.,   3.,   1.,   1.,   2.,   0.,\n",
       "        361.,  46.,   1.,   4.,   1.,   3.,   4.,   3.,   2.],\n",
       "       [  1.,   4.,   1.,  24.,  16.,   0.,   9.,   5.,   1.,   2.,   0.,\n",
       "          3., 260.,   3.,   4.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  2.,   3.,   4.,   0.,   8.,   0.,   2.,   0.,   1.,   0.,   2.,\n",
       "          2.,   6., 324.,   4.,   1.,   1.,   0.,   3.,   3.],\n",
       "       [  3.,   7.,   4.,   1.,   2.,   3.,   3.,   2.,   0.,   0.,   1.,\n",
       "          0.,   4.,   3., 336.,   0.,   2.,   0.,   7.,   5.],\n",
       "       [ 42.,   4.,   5.,   0.,   0.,   1.,   4.,   1.,   1.,   3.,   2.,\n",
       "          2.,   5.,  17.,   4., 377.,   3.,   7.,   2.,  68.],\n",
       "       [  4.,   0.,   0.,   0.,   3.,   1.,   1.,   5.,   4.,   1.,   0.,\n",
       "          8.,   0.,   3.,   1.,   3., 324.,   3.,  95.,  19.],\n",
       "       [  7.,   1.,   0.,   0.,   0.,   1.,   3.,   1.,   2.,   2.,   1.,\n",
       "          0.,   2.,   7.,   2.,   1.,   2., 325.,   4.,   5.],\n",
       "       [  7.,   1.,   9.,   0.,   6.,   2.,   5.,   8.,   5.,   8.,   3.,\n",
       "          8.,   0.,   9.,  21.,   1.,  16.,  19., 185.,   7.],\n",
       "       [  9.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.,   0.,   0.,   1.,   1.,   4.,   0.,   1.,  92.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive_bayes.ipynb",
   "provenance": [
    {
     "file_id": "1WGGHM_rTMtFlr8Wxns9bCZV9-KU-_90k",
     "timestamp": 1631093344683
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
